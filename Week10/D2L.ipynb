{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMQrRyBXOGLMz/ukQlAiAhW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoparAwayyy/Machine-Learning/blob/main/Week10/D2L.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DWI SAPUTRA SOPAR SIAGIAN\n",
        "\n",
        "1103210220\n",
        "\n",
        "D2l"
      ],
      "metadata": {
        "id": "mZanG1pu9rSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from d2l import torch as d2l\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from scipy.sparse import csr_matrix\n"
      ],
      "metadata": {
        "id": "CfT9Jl2NCIUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l # This command will install the d2l module, which seems to be required for your task."
      ],
      "metadata": {
        "id": "9eUErK9jH_pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Hwr_7z8LCDM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSL0clqf4dAb"
      },
      "outputs": [],
      "source": [
        "# Memuat dataset\n",
        "file_path1 = '/content/drive/MyDrive/ML DataSet/Week10/test.csv'\n",
        "file_path2 = '/content/drive/MyDrive/ML DataSet/Week10/train.csv'\n",
        "\n",
        "dataset1 = pd.read_csv(file_path1)\n",
        "dataset2 = pd.read_csv(file_path2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Memuat dataset\n",
        "file_path1 = '/content/drive/MyDrive/ML DataSet/Week10/test.csv'\n",
        "file_path2 = '/content/drive/MyDrive/ML DataSet/Week10/train.csv'\n",
        "\n",
        "dataset1 = pd.read_csv(file_path1)\n",
        "dataset2 = pd.read_csv(file_path2)\n",
        "\n",
        "# Define features and target\n",
        "features = dataset2.drop(columns=['Id', 'SalePrice'])\n",
        "target = dataset2['SalePrice']\n",
        "\n",
        "# Preprocessing Pipeline\n",
        "numeric_features = features.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = features.select_dtypes(include=['object']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Neural Network Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_features, train_target, num_epochs, lr, batch_size):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    train_loss = []\n",
        "\n",
        "    # Konversi sparse matrix menjadi array NumPy\n",
        "    if isinstance(train_features, csr_matrix):\n",
        "        train_features = train_features.toarray()\n",
        "\n",
        "    train_features = torch.tensor(train_features, dtype=torch.float32)\n",
        "    train_target = torch.tensor(train_target.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i in range(0, len(train_features), batch_size):\n",
        "            inputs = train_features[i:i+batch_size]\n",
        "            labels = train_target[i:i+batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return train_loss\n",
        "\n",
        "# K-Fold Cross Validation and Model Selection\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "models = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(features, target)):\n",
        "    print(f'Fold {fold+1}')\n",
        "    train_features, val_features = features.iloc[train_idx], features.iloc[val_idx]\n",
        "    train_target, val_target = target.iloc[train_idx], target.iloc[val_idx]\n",
        "\n",
        "    # Preprocess data\n",
        "    train_features_preprocessed = preprocessor.fit_transform(train_features)\n",
        "    val_features_preprocessed = preprocessor.transform(val_features)\n",
        "\n",
        "    # Initialize model\n",
        "    input_dim = train_features_preprocessed.shape[1]\n",
        "    model = Net(input_dim)\n",
        "\n",
        "    # Train model\n",
        "    train_loss = train_model(model, train_features_preprocessed, train_target, num_epochs=100, lr=0.001, batch_size=32)\n",
        "    train_losses.append(train_loss)\n",
        "    models.append(model)\n",
        "\n",
        "    # Validation\n",
        "    val_features_preprocessed = torch.tensor(val_features_preprocessed.toarray(), dtype=torch.float32)\n",
        "    val_outputs = model(val_features_preprocessed)\n",
        "    val_loss = mean_squared_error(val_outputs.detach().numpy(), val_target.values)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f'Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "# Mean Training Loss\n",
        "mean_train_loss = np.mean(train_losses, axis=0)\n",
        "print(f'Mean Training Loss: {mean_train_loss[-1]:.4f}')\n",
        "\n",
        "# Best Model Selection based on Validation Loss\n",
        "best_model_idx = np.argmin(val_losses)\n",
        "best_model = models[best_model_idx]\n",
        "print(f'Best Model Index: {best_model_idx}')\n",
        "\n",
        "# Save Best Model\n",
        "torch.save(best_model.state_dict(), 'best_model.pth')\n"
      ],
      "metadata": {
        "id": "LfD9SZx5PCrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting validation predictions vs actual values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(val_outputs.detach().numpy(), val_target.values, color='blue', label='Actual vs Predicted')\n",
        "plt.plot([min(val_target.values), max(val_target.values)], [min(val_target.values), max(val_target.values)], color='red', linestyle='--', label='Perfect Prediction')\n",
        "plt.xlabel('Predicted Sale Price')\n",
        "plt.ylabel('Actual Sale Price')\n",
        "plt.title('Validation Predictions vs Actual Values')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "an1QWFnXO8Lx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}